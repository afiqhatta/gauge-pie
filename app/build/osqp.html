<!doctype html>

<html lang="en">
<head>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-W07H38PZ62"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-W07H38PZ62');
</script>
  <meta charset="utf-8">

  <script src="https://code.jquery.com/jquery-2.2.4.min.js"></script>
  <script src="js/scripts.js"></script>
  <link rel="stylesheet" href="static/styles.css?v=1.0">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    MathJax = {
     tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
     }
    };
    window.MathJax = {
      loader: {load: ['[tex]/physics']},
      tex: {packages: {'[+]': ['physics']}}
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <meta charset="utf-8">
  
</head>

<body>
  <div class="mast">
    <h3>Afiq Hatta's blog</h3> 
  </div> 
  <div class="sidebar">
    <a href="index.html">Home</a>
    <a href="writing.html">Writing</a> 
    <a href="notes.html">Notes</a> 
    <a href="about.html">About</a> 
  </div>
  <div class="main">
    <div class="title">
      <h1> 
Operator Splitting Methods for Convex Optimisation
 
<hr class="thin">
</h1>
    </div>
     
<h3> What is a quadratic program? </h3> 

<p> 
A quadratic program is an optimisation problem of the
form 

\[ 
\begin{aligned} 
	\text{minimize} & ( 1/ 2 ) x ^ T P x + q ^ T x \\
	\text{subject to } & l \leq A x \leq u \\
\end{aligned} 
\] 

In other words, we have a quantity for which we 
want to minimise, subject to a linear set of constraints. 
In this case, the decision variable \( x \) will be 
of a given size \( n \in \mathbb { N } \), and we will have 
some \( m \in \mathbb{ N } \) constraints, along with 
\( N \) non-zero entries in the matrices \( P , A \). 
The tuple 
\( (N , n, m ) \) is typically a way to parametrise the 'size' of 
the problem we want to solve. 
</p> 

<h3> What is an interior point method? </h3> 
<p> 
Interior point methods model the constraints in an optimisation 
function as 'barriers', by pushing them up into the objection function, 
finding the minimum point of this new surface, then making these barriers steep as possible. 
</p> 

<p> 
Let's look at the one dimensional case for now. For example, if we have some function 
\( f ( x ) \) which we want to minimse subject to a single constraint 
\[ c ( x )  \geq 0  \] 
we could penalise the optimisation for going above the constraint 
by attaching a logarithmic function in the objective which
pitches up if \( c ( x) \) approaches zero. 
So, we could instead choose to maximise 
\[  B ( x , \mu )  = f ( x ) - \mu \log ( c ( x) ) \] 
And take the limit as \( \mu \to 0 \). 

</p>

<p> 
To do this, we do the following procedure. 
We rewrite the problem as 
\[ 
\begin{aligned} 
\text{min } & t f ( x ) + \phi (x ) \\
\text{subject to } & A x = b 
\end{aligned} 
\] 

We construct a path of initial points as we take the limit of \( t \) 
to be sufficiently large. 
To do this, we start with some intial value \( t = t _ 0 > \), 
and then use Newton's method to get the solution \( x ^ 0 = x ^ * ( t ) \)
to the above problem. 
We then use this new optimal value a the inital starting point 
for the problem with \( t \to \lambda t \), where \( \lambda > 1 \), 
and keep going until \( t \) is sufficiently large. 
</p> 
 
<p> 
Interior point methods are currently the defacto algorithm 
used in most commercial solvers. 
</p> 

<h3> Optimality conditions</h3> 
<p> 
We can rewrite the 
constrained problem in terms of Lagrangian 
multipliers, using a dual variable. 
As usual with the Lagrangian approach,
we will introduce \( y \) 
which encodes the constraint as 

\[ L ( x , y ) =  \( \frac{ 1 } { 2} x ^ T P x + q ^ T x + ( A x  - z ) ^ T y \] 

where we have the additional constraint \( A x =z \). 
For the optimal point, we differentiate 
both for \( x, y \) to yield the optimality condition that 
Differentiating the above conditions 

\[ \begin{aligned} 
Ax  &= z \\
P x + q + A^ T y & = 0 
\end{aligned} 
\] 
<h3> Advantages of the Alternating Direction Method of Multipliers routine </h3> 
<p>  
The alternating direction method of multipliers (ADMM) routine 
has been popular lately due to its good convergence 
propertires. It also is known to 
provide decent accuracy quite cheaply. 
However it is typically unable to detect unfeasibility in a problem, and it 
is currently also unclear how to select optimal step size parameters to 
get an optimal convergence rate. 
</p>
<h3> Solution using the alternating method of multipliers </h3> 
<p> 
With the ADMM method, we take a slightly different approach. 
In the above scenario, \( x \in \mathbb{ R } ^ n \) is our control variable, 
and \( z \in \mathbb{ R } ^ m \) encodes our constraints. 
We reformat the problem by imposing a new constraint that 
two auxillary variables \( \bar{ x}, \bar{z } \) are equal to 
\( x, z \) respectively, and push the constraint \( A x  = z \) 
into the objective function with an indicator function. 
</p> 

<p> 
The first step of the ADMM algorithm states that 
we have to solve the problem 

\[  
\text{minimise }   (1 / 2) \bar {x } ^ T P \bar {x } + q ^T \bar { x } + ( \sigma / 2 ) 
| \bar { x }  - x ^ k | ^ 2 + ( \rho / 2 ) | \bar { z }  - z ^ k + \rho ^ { - 1} y ^ k | ^ 2   
\]

subject to \( A \bar { x } = \bar { z } \). 
 
We solve this problem with Lagrange multipliers, 
where the control variables are \( \bar { x } , \bar{ z } \). 
We attach a Lagrange multiplier \( v ^ k \) to the constraint 
\( A \bar { x }  = \bar { z  } \), which gives us the 
system of KKT equations,  
Typically, we can prefactorise this matrix for the whole term, and then use this factorisation 
repeatedly  in each iteration. 
</p> 

<h3> Certifying feasibility with ADMM</h3> 
<p> 
In Heinz and Combettes' textbook on 
Convex analysis, proposition 6.46 states that 
if \( C \) is a closed conves subset of some Hilbert space \( H \), 
then we have that the projection into the cone, \( \Pi \), 
satisfies 

\[ p = \Pi ( x ) \iff x - \Pi (x) \in N _ C ( p ) \] 
\( N _ C ( p ) \) is the normal cone and is defined as 
the set 

\[ N _ C ( p )  = \{ u \in H \mid \sup ( C - p \mid u ) \leq 0 \] 
</p> 

<p> 
In theorem 3.14, in the first edition of his book, 
he shows that this is true. 
This means that in the ADMM algorithm, 
we find that the iterates \( z ^ k, y ^ k \), satisfy the condition 
that they are in the right sets. 
</p> 

<h3> Active constraints </h3> 
<p> 
An active constraint, if there is just a single constraint, 
is a constraint in which the optimal point would be different without it. 

</p> 

<h3> Pre-conditioning </h3> 
<p> 
Methods like these tend to have unpredictable 
convergence rates when problems are badly scaled. 
There is also the question of the optimal choice of parameters 
to speed up convergence. 
A useful measure of how inaccurate matrix inversion 
operations are is the matrix <b>condition number</b>. 
</p> 

<p> 
Not including round-off error 
from floating point arithmetic, the condition number of a matrix 
associated with solving a system such as \( A x = b\). 
</p> 

<p> 
In general, if we have a vector 
 \( b \), we expect some intrinsic error associated with it. 
Let's call this error vector \( e \). More often than 
not, we care more about how big this error vector is 
relative to \( b \) than we do about the absolute size itself. 
So, we can quantify the relative error as 

\[ \text{rel}_{\mathbf { b } }  = \frac{ | e | }{ | b | } \] 
Similarly, the error in the solution to the equation above relative to 
the solution itself can be written as 
\[ \frac{ |  { A ^ { -1 } e } | } { | A ^ { - 1} b | } / \frac{ | e | } { | b | } \] 
This ratio motivates the definition of 
a condition number, which is defined as 

\[\kappa ( A ) = \max \{ \frac{ | A ^ { -1 } e | } { | e | } \frac{ | b | } { | A ^ { - 1} b | } \} 
 = \max \{ \frac{ | A ^ { -1 } e | } { | e | } \} \max \{ \frac { | A x | } { | x | } \}  
 = | A ^ {  1- } |  |A | \]  
</p> 

<p> 
 
<br> 
<br> 
<br> 



  </div> 
</body>
</html>