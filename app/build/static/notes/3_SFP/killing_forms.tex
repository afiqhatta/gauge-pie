\section{The Killing Form}
We will now create a structure on 
our Lie algebra which is somewhat analogous to a 
metric on vectors (which we covered 
in the general relativity notes). 
Given a vector-space like Lie algebras, 
it is of interest for us to define a scalar product 
on the Lie algebra, which takes two vectors and returns a scalar. 

Let's first define an inner product first. 
\begin{defn}{(Inner product)} 
	Given a vector space $ V $ over a field $ F $, an inner product is 
	a bilinear map 
	\[
	 i : V \times V \to F 
	\] where this map is symmetric. This is notion is 
	analogous to the usual dot product we all know and love, 
	but we've made things a bit more general. 
\end{defn}
Right now, we'll define the inner product in this way. 
However, we'll want to make an extra definition, 
motivated by the fact that we don't care about inner 
products where, when we contract a specific element with 
all others in a lie algebra, we get zero. 
\begin{defn}{(Non-degeneracy)} 
	 We say that the inner product $ i $ is non-degenerate 
	 if for all $ v \in V ( v \neq  0 ) $, there 
	 is a $ w \in V $ such that $ i ( v, w ) \neq 0$. 
	 With this, our concept of an inner product looks more like a metric. 
\end{defn}
Now, a natural question 
to pose is if there is a natural inner product which we can 
write down for a Lie algebra? What does the term 'natural' 
even mean? We'll answer this question later. 
For the first question The answer is yes; our answer is the \textbf{killing form}. 
\begin{defn}{(Killing form)} 
	Our Killing form is a map from the Lie algebra $ \lalg $
	 \[
	 \kappa : \lalg \times \lalg \to F 
	\] defined as the map which takes two $ X , Y  \in \lalg $, 
	which takes 
	\[
	 K ( X, Y )  = \tr ( \ad_ X \circ \ad _ Y ) 
	\] So, we are taking 
	the trace of the composition of tow adjoint 
	representations. By the cyclic property of trace, we have that 
	this object is symmetric since we 
	can switch the adjoint maps around and still have the same map. Hence, 
	it's still and inner product. 
	In addition, since the adjoin maps are linear in both arguments, 
	we have that this map is linear. 
\end{defn}
Now, what does this map look like 
in terms of a basis of our Lie algebra? Let's look at the map 
\[
 ( \ad_ X \circ \ad_ Y ) : \lalg \to \lalg 
\] we write out the definition of the 
adjoint map explicitly, which is the commutator. 
Composing two commutator operations means that some $ Z $ is mapped 
as
\[
 Z \in \lalg \to [ X , [ Y , Z ] ] \in \lalg
\] What is the matrix representation of this map? 
To do this, we construct a basis $ \left\{  T ^ a : a = 1, \dots , D  \right\} $ 
for $ \lalg$. 
Writing out our components, and our structure constants explicitly, 
 \[
 X  = X_ a T ^ a , \quad 
 Y = Y  _a Y ^ a , \quad
 Z = Z_ a T ^ a, 
 \quad [ T ^ a , T ^ b ] = f ^{ ab } _ c T ^ c 
\] Multiplying components out, 
we find that 
\begin{align*}
	[ X. [ Y , Z ] ] &=  X_a Y _ b Z_ c [ T ^ a , [ T ^ b , T ^ c ]] \\
	&=  X_ a Y _ b Z _ c f ^{ ad } _ e f ^{ bc } _ d T ^ e  \\
	&=  M ( X, Y ) ^ c _ e  Z_ c T ^ e  \\
\end{align*} 
In this expression, we have that 
\[
 M ( X , Y ) ^ c _ e  = X_ a Y _ b f^{ ad } _ e f ^{ bc } _ d 
\] Taking the trace of this map, we 
find the components explicitly by 
taking the trace 
\[
 K ( X, Y ) = \tr _ D [ M ( X, Y ) ] = K ^{ ab } X_ a Y _ b 
\] where $ \kappa ^{ ab } = f ^{ ad } _ c f ^{ bc } _  d$. 

What does the term natural mean? 
It means that the map $ \kappa $ should be invariant under the 
adjoint action $ \lalg $. This action condition 
is given by 
 \[
 \kappa ( [ Z, X] , Y ) + \kappa ( X, [ Z, Y ] )  = 0, \forall X, Y , Z \in \lalg 
\]
We now prove this invariance condition. 
\begin{thm}{($ \kappa $ is invariant under the adjoint action ) } 
	Writing out the term on the left explicitly, 
	\[
		\kappa ( [ Z, X] , Y ] )  = \tr [ \ad _{ [ Z , X ] } \circ \ad_Y ] 
	\] the defining property of our adjoint 
	representation is that 
	\[
		\ad_{ [ Z, X ] }  = ( \ad _ Z \circ \ad_ X - \ad _ X \circ \ad _ Z ) 
	\] This means that the above reads 
	\[
	 \dots = \tr [ \ad_ Z \circ \ad_ X \circ \ad_ Y ] - \tr [ \ad_ X \circ 
	 \ad_ Z \circ \ad_ y ] 
	\] Similarly, applying this to the second term, 
	we find that 
	\[
	 \kappa ( X, [ Z, y ] )  = 
	 \tr [ \ad_ X \circ \ad _ Z \circ \ad_ Y ] - \tr [ \ad_ X \circ \ad_ Y \circ \ad_ Z ] 
	\] By cyclicity of the trace, this evaluates to zero. 

\end{thm}


Given a matrix Lie group $G$, we can define the 
adjoint action of $ G $ on $ \mathcal{ L } ( G ) $. 
For all $ g \in G $, since we're dealing with matrix Lie 
groups, we can do inverses and multiply Lie algebra elements with 
$ g  \in G $. The action is defined as 
\[
	X \in \mathcal{ L } ( G ) \mapsto g X g^{ - 1 } \in \mathcal{ L }  (G ) 
\] Our Killing form is 
invariant of this action. In particular, we have that 
\[
	K ( X, Y ) = \kappa ( g X g^{ - 1} , g Y g^{ - 1 } ) ,\quad
	\forall X, Y \in \mathcal{ L } ( G ) , \forall g \in G 
\] We can recover the invariance property
for Killing forms by writing 
$ g = \text{Exp}( - t Z ) $. Expanding out, we get 
the invariance property. 


\begin{defn}{(Semi-Simple)} 
	A Lie algebra $ \lalg $ is semi-simple if it has no Abelian ideals. 
\end{defn}

\begin{thm}{(Cartan)}
	We have that $ \kappa $ is a non-degenerate Killing form $ \iff $ $ \lalg $ 
	is semi-simple. 
	We will prove later on that any semi-simple $ \lalg $ is a direct sum of 
simple Lie algebras, 
so we can decompose $\lalg$ is 
\[
\lalg = \lalg_ 1 \oplus \lalg_ 2 \oplus \dots \oplus \lalg_{ m }\]
\begin{proof}
	We will show first that $ \lalg $ is not semi-simple implies that
	$ \kappa $ is degenerate. 
	If $ \lalg $ is not semi-simple, then it has an Abelian ideal, 
	which we will call $ \mathcal{ J } \subset \lalg$, with 
	$\dim ( \lalg ) = D $ and $ \dim ( \mathcal{ J } )  = d \leq D $. 
	Construct a basis of the algebra $ \left\{  T^ a  \right\} $ with 
	$ a  = 1 , \dots D $, which we write as 
	\[
	 \mathcal{ B } = \left\{  i = 1 , \dots d  \right\}  \cup \left\{ 
	 T ^{\alpha } , \alpha = 1, \dots D - d\right\} 
	\] where we have $ T ^ i $ spanning $\mathcal{ J  } $. 
	As $ \mathcal{ J } $ is Abelian, we have by definition 
	that 
	\[
	 [ T ^ i , T ^ j ] = 0 , \forall i,j 
	\] We also have that since it's an ideal, 
	\[
	 [ T ^ \alpha , T ^ j ] = f ^{ \alpha j } _ k T ^ k \in \mathcal{ J }
	\] We thus have 
	\[
	 f^{ ij } _ a = 0 , \quad f^{ \alpha j } _ \beta = 0 
	\] Now suppose that we have 
	$ X = X_ a T ^ a \in \lalg  $,and $ Y = Y _ i T ^ i \in \mathcal{ J } $. 
	If we write out our Killing form 
	\[
	 K [ X, Y ] = K^{ ai } X_ a Y_ i
	\] then we have that 
	\begin{align*}
		\kappa ^{ ai } & = f ^{ ad }_ c f ^{ ic } _ d \\
		&=  f ^{ aj } _ \alpha f ^{ i \alpha } _ j  \\ 
		&= 0  
	\end{align*} 
	Thus, for all $ Y \in \mathcal{ J } $ we have that 
	\[
	 \kappa ( Y , X ) = 0 , \quad \forall X \in \lalg 
	\] which implies that this killing form is degenerate. 
\end{proof}
\end{thm}

\section{The Cartan Classification}
in this section, we will classify all finite dimensional, 
simple, complex $\lalg$. This is based on work done by Cartan in 1894. 
When we were looking at the Lie algebra $\mathcal{ L } ( SU ( 2) ) $, 
we chose the convenient basis $ \left\{  H, E_- , E_+ \right\} $ because 
$ H $  was nice and diagonal. But, there's a deeper reason 
why we chose this basis - it's part of a wider class of 
bases which have really nice properties. 
\begin{defn}{(Ad-diagonalisability)}
	We say that a Lie algebra element $ X \in \lalg $ is 
	ad-diagonalisable (AD) if the map 
	\[
	 \ad_ X : \lalg \to \lalg \text{ is diagonalisable }
 \] In the case of $ \mathcal{ L } ( SU ( 2) ) $, this was 
 our Cartan element $ H $. 
\end{defn}
 
We can turn the set of ad-diagonalisable elements into 
a subalgebra of our Lie algebra. This is called the Cartan subalgebra. 

\begin{defn}{(Cartan Subalgebra)}
A Cartan subalgebra $ \subalg \in \lalg $ is a maximal abelian subalgebra 
consisting of AD elements. Recall that what we mean 
by abelian is that the vectors in the Lie algebra commute 
under the Lie bracket. 
This is defined as follows. 
\begin{enumerate}
	\item If $ H \in \subalg \implies H  $ is ad-diagonalisable by definition.  
	\item $ H , H ' \in \subalg \implies [ H , H ' ] = 0$. 
	\item If  $ X \in \lalg $ and $ [ X, H ] = 0 , \forall H \in \subalg$, 
		then we necessarily have that  $ X \in \subalg$. 
\end{enumerate}
In fact, we have that all possible Cartan subalgebras of $ \lalg $ are 
isomorphic and have the same dimension. 
\[
 r = \dim [ \subalg ] \in \mathbb{ N} \text{ defined as the rank of } \lalg 
\] 	
\end{defn}

\begin{example}
	Consider the Lie algebra $\lalg  = \mathcal{ L }_{ \mathbb{ C} } ( SU ( 2) )  = 
	\text{span}_{ \mathbb{ C} } \left\{  H , E_ - , E _ +  \right\} $. 
	We only have one diagonal element, where $ H  = \sigma _ 3 $  is AD. 
	Specifically, \[
		[ H , E_{ \pm } ] = \pm 2 E_{\pm}, [ H , H ] = 0 
	\] Observe that however, $ E_{ \pm } $ are not diagonalisable. 
	These are not AD. 
\end{example}
Let's consider another example. 
\begin{example}
	Suppose that $\subalg  = \text{span}_{ \mathbb{ C} } \left\{  H  \right\} $ 
	is a choice of a Cartan subalgebra. 
	Choose a basis $\left\{  H ^ i, i =  1, \dots r  \right\} $, 
	such that $ [ H ^ i , H ^ j ] = 0 \forall i , j $. 
	Consider  $ \mathcal{L}_{ \mathbb{ C} } ( SU ( N ) )$, the set 
	of traceless $ n \times n $ matrices. 
	Now, diagonal elements of $ \mathcal{ g } $ provide a choice 
	of Cartan subalgebra! Take $ \alpha, \beta  = 1 , \dots N$. 
	We can construct this basis by choosing 
	 \[
	 \left(  H ^ i  \right)_{ \alpha \beta }  = 
	 \delta _{ \alpha i } \delta _{ \beta i }  - \delta _{ \alpha , i + 1 } \delta _{ \beta , 
	 i + 1 }, \quad i  = 1 , \dots , N - 1
 \] This shows that our rank $[\mathcal{ L }_{ \mathbb{ C} } ( SU ( N ) )  = N -1$. 
 We have that our basis elements $ [ H ^ i , H ^ j ] = 0, \forall i , j  = 1 , \dots r $. 
 By the property of the adjoint representation, we have that 
 \[
  \left(  \ad_{H ^ i } \circ \ad_{ H ^ j }  - \ad_{ H ^ j } \circ \ad_{ H ^ i }  \right) = 0  
 \] This implies $ r $ linear maps $\ad_{ H ^ i } : \lalg \to \lalg $ are simultaneously 
 diagonalisable. So $ \lalg $ is spanned by the simultaneous eigenvectors of $ \ad_{ H _ i }$.
 \[
	 \ad_{ H ^ i } ( E ^{ \alpha } ) = [ H ^ i , E ^ \alpha ] = \alpha ^ i E ^ \alpha , 
	 \quad i = 1 , \dots,  r 
 \] 
\end{example}

This allows us to define a basis 
called the \textbf{Cartan Weyl} basis, which 
is a basis constructed from the diagonal elements as 
well as the step down and step up operators. 

For example for $ \lalg  = \mathcal{ L } _ \mathbb{ C}  ( SU ( 2) ) $, 
our Cartan-Weyl basis was the set $\left\{  H , E_ - , E _ +  \right\} $, 
with the diagonalisable element being $ H $, with the step operators 
being $ E _{ \pm } $. 

We define something more general called a \textbf{Cartan Subalgebra}. 
The Cartan subalgebra is a subalgebra $ \subalg \subset \lalg $ ,
of dimension $ r = \text{Rank} [ \lalg ] $. 
We construct a basis of $ \subalg $ as 
\[
	\left\{  H ^ i , i =1 \dots r  \right\} , \quad [ H ^ i , H ^ j ]  =0 \quad \forall 
	i , j 
\] In this case, we can try to diagonalise the adjoint
action for each of these $ H ^ i $. This is possible, by the defining 
property of representations.
 $ \lalg $ is spanned by the simultaneous eigenvectors of the ad maps 
 \[
  \ad_{ H ^ i } : \lalg \to \lalg 
 \] We can see that this is 
 true for $ \mathcal{ L } ( SU ( 2) ) $.
 Starting off with zero eigenvalues, the eigenvectors are just in the Cartan 
 subalgebra themselves, in other words the set $\left\{ H ^ i , i = 1 , \dots ,r   \right\} $. 
 This is because by construction, we have that 
 \[
	 \ad _{ H ^ i } ( H ^ j ) = [ H ^ i , H ^ j ] = 0 \quad \forall i , j 
 \] Now, what about non-zero eigenvectors? 
 Consider the indexed set $ \left\{  E ^ \alpha , \alpha \in \Phi  \right\} $. 
 Because this is as eigenvector, we have that 
 \[
	 \ad_{ H ^  i } ( E ^{ \alpha } ) = [ H ^ i , E ^{ \alpha } ] = \alpha ^ i E ^ \alpha  
 \] Since we're working in a complex basis, we must have that 
 $ \alpha ^ i \in \mathbb{ C} $, which are not all zero (otherwise it 
 would lie in the Cartan subalgebra). We call the objects $ \alpha $ as 
 \textbf{ roots } of the Lie algebra. 

 Now lets say a little bit more about the roots. 
 For a general element of the Cartan subalgebra $ \subalg $, 
 $ H \in \subalg$. As we have a basis of the Cartan subalgebra, 
 we can write 
 \[
  H = e_ i H ^ i , \quad e _ i \in \mathbb{ C} 
 \] By linearity, we have that the bracket of $ H $ with a step generator 
 can be expanded, as follows. 
 \[
	 [ H , E ^ \alpha ] = \alpha ( H ) E ^{\alpha } , \quad \alpha ( H ) = e_ i \alpha ^ i \in 
	 \mathbb{ C} 
 \] This is because, upon expanding $ H $, we get that 
 \begin{align*}
	 [ H , E ^ \alpha ] &=   [ e_ i H ^ i, E ^ \alpha ]  \\
			    &= e_ i [ H ^ i , E ^ \alpha ]  \\
			    &=  e_i \alpha ^ i E ^ \alpha  \\
			    &=  e _ i \alpha ^ i E^ \alpha  \quad 
			    \text{ define } e_ i \alpha ^ i  = \alpha ( H ) \in \mathbb{ C}  \\ 
 \end{align*}Thus, a more sophisticated way to think about roots
 is to think of them as $ \alpha : \subalg \to \mathbb{ C}  $. 
 Thus, they're elements of the dual vector space 
 $ \alpha \in \subalg ^ * $, where we write $ \subalg ^ * $ as 
  the dual space. Now we may have degeneracy in this case,
  but we can prove that the roots are non-degenerate if 
  $ \lalg $ is simple. However, we will not prove this in these
  notes. 
  This means that the set of roots $ \Phi$ consist of  $ d - r $ 
  distinct vectors by non-degeneracy, which are distinct elements 
  of $ \subalg ^ * $. This is for $ d  = \dim [ \lalg ] $, $ r = \dim [ \subalg ] $. 

  \begin{defn}{(Cartan Weyl Basis)}
  	We can write the basis of the Lie algebra in terms
	of ad-diagonalisable elements and the step operators. 
	\[
	 \mathcal{ B } = \left\{  H ^ i , i = 1, \dots r   \right\} \cup 
	 \left\{  E ^ \alpha , \alpha \in \Phi  \right\} 
	\]  
  \end{defn}
 So far, we haven't really used the assumption that the Lie algebra is simple ( 
 other than for the non-degeneracy condition). 
Using Cartan's theorem, we remind ourselves 
that $ \lalg $ being simple implies that the Killing form 
\[
	\kappa ( X, Y ) = \frac{1}{N } \tr [ \ad _ X \circ \ad_ Y ] 
\] is non-degenerate. 
With this, we can play around with the structure 
of the Killing form in the Cartan-Weyl basis. 
We'll start by proving two things.
\begin{thm}
	\begin{enumerate}
		\item $ \forall H \in \subalg , \alpha \in \Phi $, we have 
			that the Killing form vanishes for 
			\[
			 \kappa ( H , E ^ \alpha ) = 0 
			\] This is somewhat 
			of a nice statement because we can interpret this 
			as the vectors $ E ^{ \alpha } $ being 
			orthogonal to the vectors in our Cartan subalgebra $ H$. 
		\item $ \forall \alpha, \beta \in \Phi $, and $ \alpha + \beta \neq 0 $, 
			we have that 
			\[
			 \kappa ( E ^ \alpha, E ^ \beta ) = 0 
			\] This is somewhat of a non-obvious condition! 
	\end{enumerate}
\begin{proof}
Throughout this proof, we'll use 
Jacobi identity, the adjoint representation, and Killing form 
invariance. 
We'll prove the first statement. 
\begin{enumerate}
	\item Consider $ \alpha ( H ' ) \kappa ( H , E ^ \alpha )$ for some 
		arbitrary $ H ' $ in the Cartan subalgebra. Note that, 
		we can't possibly have $ \alpha ( H ' )  =0 $ for 
		all  $ H' $ in the subalgebra because, by maximality, 
		this would then imply that $ [ H' , E ^ \alpha ]  = 0 $ 
		for all $ H'  $ , which implies that $ E ^ \alpha $ was 
		in the subalgebra (false by assumption). 
		By linearity, we pull the root side the killing form 
		in the first line. We have that 
		 \begin{align*}
			 \alpha ( H ' ) \kappa ( H , E ^ \alpha ) &=  
			 \kappa ( H ,  [ H ', E ^ \alpha ] )  \\
								  &=  - \kappa ( [ H ' , H ] ,
								  E^ \alpha ) \text{ by the invariance 
								  property of the Killing form }\\
								  &=  - \kappa ( 0 , E ^ \alpha )  \text{ 
								  by the Abelian property of the 
							  Cartan subalgebra }\\
								  &=  0  
		\end{align*}
		Now, $ \alpha ( H ' ) \neq 0 $ from $ H ' $, implies 
		that 
		 \[
			 \kappa ( H , E ^{ \alpha } ) = 0 
		 \] This proves the first statement. 
	\item Again, consider $ H ' \in \subalg $. Now, consider 
		\begin{align*}
			( \alpha ( H ') + \beta ( H ' ) ) \kappa ( E ^ \alpha , 
			E ^ \beta )  &=  \kappa ( [ H ' , E ^ \alpha ] , E ^ \beta )  
			+ \kappa ( E ^ \alpha , [ H ' , E ^ \beta ] ) \\
			 &=  0  \text{ by the invariance property} 
		\end{align*}
		Now, provided we assume that $ \forall \alpha, \beta \in \Phi$, 
		with $ \alpha + \beta \neq 0 $ then by maximality that 
		$ \alpha ( H ' ) + \beta ( H ' ) \neq 0 $  for some $ H ' $. 
		This in turn then means that the other 
		factor must be zero and hence we have that 
		\[
		 \kappa ( E ^ \alpha , E ^ \beta ) = 0 
		\] 
	
\end{enumerate}
\end{proof}
\end{thm}

\begin{thm}{(Non-degenracy of the Killing form result)} 
	We have that for all $ H \in \subalg $, there exists 
	a $ H ' $ such that $ \kappa ( H , H ' ) \neq 0 $. 
\begin{proof}
	For some $ H \in \subalg $, assume the converse that 
	$ \kappa ( H , H ' ) = 0 , \quad \forall H ' \in \subalg  $. 
	From i), we have that $ \kappa ( H , E ^ \alpha) = 0 , \quad \forall \alpha \in 
	\Phi $. This means that $ \kappa ( H , X )  = 0 $, for all 
	$ X \in \lalg $, which implies that $ \kappa $ is degenerate. 
	Contradiction. 
\end{proof}
The significance of this theorem is that 
we've shown that $ \kappa $ is not only non-degenerate on the whole space, 
but is also a non-degenerate inner product on $ \subalg $. 
In particular, if we write the inner product for any two 
elements $ H , H ' $ as 
\[
 \kappa ( H , H' ) = \kappa ^{ ij } e _ i e' _ j, \quad H = e_ i H ^ i, H ' = e ' _ i H ^ i  
\]  The condition of non-degeneracy implies that 
$ \kappa ^{ ij }  = \kappa ( H ^ i , H ^j ) $ is an invertible $ n \times n $ 
matrix. This means that we can find an inverse. 
\[
	\exists ( \kappa ^{ -1 } )_{ ij } \implies ( \kappa ^{ - 1} ) _{ ij } \kappa ^{ jk} 
	 = \delta ^ k _ i 
\] Thus, like in general relativity, it's natural to think 
about lowering indices and transferring things over to the dual space. 
\end{thm}
Now, $ \kappa ^{ -1 }  $ defines a non-degenerate inner product 
on the dual space $ \subalg ^ * $. Suppose we have a root $ \alpha $ so 
that 
\[
 [ H ^ i , E ^ \alpha ] = \alpha ^ i E ^ \alpha, \quad [ H ^ i, E ^ \beta ] = \beta ^i E ^ \beta 
\] We can now define the inner product $ ( \alpha, \beta ) $, as 
\[
	( \alpha, \beta ) = ( \kappa ^{ - 1} ) _{ ij} \alpha ^ i \beta ^ j 
\] 
\begin{thm}{(A result on the roots)} 
	If we have $ \alpha \in \Phi $, then necessarily we have that
	$ - \alpha \in \Phi $ , and that $ \kappa ( E ^ \alpha , E ^ - \alpha ) \neq 0$. 

\begin{proof}
	From i), we have that $ \kappa ( E ^ \alpha , H )   = 0 , \forall H \in \subalg $. 
	From ii), we have that $ \kappa ( E ^ \alpha , E ^ \beta )  = 0, \forall 
	\beta \in \Phi $ with $ \alpha \neq - \beta $. 
	Hence, unless $ - \alpha \in \Phi $, and $ \kappa ( E ^ \alpha , E ^{ - \alpha }  ) \neq 0 $, 
	we have that  $ \kappa ( E ^ \alpha , X ) = 0 , \quad \forall X \in \lalg $. This
	implies that $ \kappa $ is degenerate. 
\end{proof}
\end{thm}

\subsection{Algebra in the Cartan Weyl basis}

\subsubsection{Working out the Bracket of $ \left[  E ^ \alpha, E ^ \beta  \right]  $}
In this section, we'll work on fully fleshing 
out the Lie algebra relations in our Cartan Weyl basis. 
Let's summarise some of the relations which we we have so far. 
By construction, elements in the Cartan subalgebra commute, 
and we've defined eigenvectors with weights. 
\begin{align*}
	[ H ^ i , H ^ j ] &=  0 , \quad \forall i , j   1, , \dots, r  \\ 
	[ H ^ i , E ^ \alpha ] & = \alpha ^ i E ^ \alpha \quad \forall \alpha \in \Phi 
\end{align*}
The only thing we've yet to do 
is explore what happens when we 
commute two of the eigenvectors, say $  E^ \alpha$ and $ E ^ \beta $. 
It remains to evaluate $ [ E ^ \alpha , E ^ \beta ] , \forall \alpha, \beta \in \Phi$.
This is an eigenvector of elements in the Cartan subalgebra. 
Using the Jacobi identity, 
 \[
 [ H ^ i , [ E ^ \alpha, E ^ \beta ] ] =  - [ E ^ \alpha , [ E ^ \beta , H ^ i ]] - [ E ^ \beta , 
 [ H ^ i , E ^ \alpha ] ]  = ( \alpha ^ i + \beta ^ i ) [ E ^ \alpha , E ^ \beta ] 
\] 
This means that we get one extra bracket to our set of relations.
Now, if $ \alpha + \beta \neq  = 0  $, then we 
have a non-zero eigenvalue here, and by non-degeneracy of eigenvalues
this implies that $ \left[  H ^ I , \left[  E^ \alpha, E ^ \beta  \right]   \right]  $
is proportional to the $ \alpha + \beta $ eigenvector. This, 
If $ \alpha + \beta \neq 0 $, 
\[
 [ E ^ \alpha , E ^ \beta ] = N_{ \alpha, \beta  } E ^{ \alpha + \beta }, \quad \text{ if } 
  \alpha + \beta  \in \Phi 
\] or is $ 0 $ otherwise. By the way, 
we could also have that $ N_{ \alpha, \beta } $ is also zero. 
If This means that 
putting a bracket around two step operators 
gives the summed step operator only if the roots are in 
the right place. 
Now, what happens when $ \alpha + \beta  = 0 $? 
Then, we learn that 
\[
	[ H ^ i , [ E ^ \alpha , E ^{ - \alpha } ]]   = 0 , \quad \forall i \in 1, \dots r 
\] This implies that actually $ [ E ^ \alpha , E ^{ - \alpha } ] \in \subalg $, 
since our Cartan subalgebra is defined to be the maximal 
subalgebra of vectors which commute with each other.

\begin{example}{(Relating this back to $ \mathcal{ L } ( SU ( 2) ) $}
We have shown that when we commute 
two vectors with eigenvalues that are negatives of each other, 
we get an element that's right back in the 
Cartan subalgebra.
Note that this is exactly what we had 
in the case of $ \mathcal{ L } _ \mathbb{ C} ( S U( 2) ) $
We can compare this with the structure of 
the complexified Lie algebra $ \mathcal{ L } _{ \mathbb{ C} } ( SU ( 2) ) $, 
where commuting $ E_ + $ and $ E _ - $ gives us back $ H $. 
\end{example}

So, we've found that $ [ E ^ \alpha , E ^{ - \alpha }  ] $ is 
in our Cartan subalgebra, but what exactly is it? 
Define the following normalised element 
\[
	H ^ \alpha  = \frac{ [ E ^ \alpha , E ^{ - \alpha }  ] }{ 
	\kappa ( E ^ \alpha , E ^{ - \alpha } ) }
\] To figure out $ H ^ \alpha $, plug it into the Killing form 
\begin{align*}
	\kappa ( H ^ \alpha , H ) &=  \frac{1}{\kappa ( E ^ \alpha , E ^{ - \alpha } )  } 
	\kappa ( [ E ^ \alpha , E ^{ - \alpha } ] , H ) \\
				  &=  \frac{1}{\kappa ( E ^ \alpha , E ^{ - \alpha } ) } 
				  \kappa ( E ^ \alpha , [ E ^{ - \alpha } , H ] ) \\
				  &=  \alpha ( H ) \frac{
				  \kappa \left(  E ^ \alpha , E ^{ - \alpha }  \right) }{ 
			  \kappa \left(  E ^{ \alpha  } E ^{ - \alpha } \right) } \\
\end{align*} 
This means that ultimately we have that 
\[
	\kappa \left(  H ^ \alpha, H  \right)   = \alpha \left(  H  \right) 
\] From our result that $ \kappa $ is a non-degenerate 
Killing form on the Cartan subalgebra, 
the can try to invert this equation. 
In components, we have that 
\[
 H^ \alpha ,  = e_ i ^ \alpha H ^ i , \quad H  = e _ i H ^ i \in \subalg
\] This implies that, in components, 
\[
	\kappa ^{ ij } e _ i ^ \alpha e _ j  = \alpha ^ j e _ j , \implies \kappa ^{ ij } e _ i ^ \alpha 
	= \alpha ^ j  \implies e _ i ^ \alpha  = ( \kappa ^{ - 1} ) _{ ij } \alpha ^ j 
\] This means that, putting everything back together, 
\[
 H ^ \alpha = e _ i ^ \alpha H ^ i  = \left(  \kappa ^{ - 1}  \right)  _{ ij  } \alpha ^ j H ^ i 
\]Let's summarise what's going on here. 
\[
	[ E ^ \alpha , E ^ \beta ]  =\begin{cases}
		N_{ \alpha, \beta } E ^{ \alpha + \beta } & \alpha + \beta \in \Phi \\
		\kappa ( E ^{ \alpha } , E ^{ - \alpha } ) H ^ \alpha & \alpha + \beta  = 0 \\
		 0 & \text{otherwise}
	\end{cases}
\] 
\subsubsection{Connections with $ \mathcal{ L } ( SU ( 2) ) $} 
Now, to go further with this, 
and to restrict the structure even further, 
we need to exploit next our understanding 
of the representation theory of $\mathcal{ L } ( SU ( 2) ) $. 
There's actually an $ \mathcal{ L } ( SU ( 2) ) $ subalgebra 
hiding in the structure of the roots, and using 
this we can start to pin down the root systems. 
Consider the bracket $ H ^ \alpha \in \subalg$, 
for all  $ \alpha, \beta \in \Phi $
\begin{align*}
	[ H ^ \alpha , E ^ \beta ] &=  \left(  \kappa ^{ - 1 }  \right) _{ ij } 
	\alpha ^ j [ H^ i , E^ \beta ] \\ 
				   &=  \left(  \kappa ^{ - 1}  \right)_{ ij  } 
				   \alpha ^ i \beta ^ j E ^ \beta  \\
				   &=  ( \alpha , \beta ) E ^ \beta  
\end{align*}
In the last line we have an 'inner product' 
given by our Killing form. 
We now define a rescaling $ \forall \alpha \in \Phi $, 
\[
	e ^ \alpha  = \frac{\sqrt{2}  }{\left(  
	( \alpha, \alpha ) \kappa \left(  E ^ \alpha, E ^{ - \alpha }  \right)  \right) ^{ \frac{1}{2 } } }, 
	\quad h ^ \alpha  = \frac{2}{\left(  \alpha, \alpha \right)  } H ^ \alpha 
\] here we have not proven that $ \left(  \alpha , \alpha  \right)   \neq 0 $. 
See Fuchs, page 87 for a proof of this. 
For all $ \alpha , \beta \in \Phi $, we have that using 
our normalised vectors, 
\begin{align*} 
 [ h ^ \alpha , h ^ \beta ] &=  0  \\ 
 [ h ^ \alpha ,  e ^ \beta ] &=  \frac{ 2 \left(  \alpha, \beta  \right)  }{ \left(  
 \alpha , \alpha \right)  } e ^ \beta  \\
 [ e ^ \alpha , e ^\beta ] &=  n_{ \alpha, \beta } e ^{ \alpha + \beta } \quad \alpha + \beta \in \Phi  \\ 
 &=  h ^ \alpha \quad \alpha + \beta = 0  \\ 
 &=  0  \text{ otherwise }\\ 
\end{align*} 
We've shown that $ \alpha \in \Phi \implies  - \alpha \in \Phi $. 
We can look at the triplet of elements $ \left\{  h ^ \alpha , e ^ \alpha , e ^{ - \alpha } \right\}  $. 
Now, in the above equation, if we set $ \beta = - \alpha $, 
we find that 
\[
	[ h ^ \alpha, e ^{ \pm \alpha } ] = \pm e ^{ \pm \alpha }
\] The point is, if we specialise 
the brackets to these three elements, 
they take a very special form. We also have that 
\[
 \left[  e ^{ \alpha } , e ^{ - \alpha }  \right]   =h ^ \alpha 
\] Hence, we have an $ \mathcal{ L } _{ \mathbb{ C} } ( SU ( 2) ) $ 
subalgebra for each root! 
Now note, than in general, the generators for the $ SU ( 2) $ Lie 
algebra may not commute with each other - they're not 
commuting subalgebras. We will call 
the subalgebra generated by a given root $ Sl ( 2 )_ \alpha $. 
In fact, we can use our knowledge about simple 
finite dimensional representations of $ \mathcal{ L } ( SU ( 2) ) $
to constrain values of the root. 

\subsection{Consequences} 
For any $ \alpha , \beta \in \Phi  $ where  $ \alpha \neq \pm  \beta $, 
we define the '$ \alpha $-string passing through $ \beta $ ' 
as the set of all 
roots from the form $ \beta + e \alpha $ for some
$ e \in \mathbb{ Z} $. This is the set 
\[
	S_{ \alpha, \beta } = \left\{  \beta + e \alpha  \in \Phi , e \in \mathbb{ Z}  \right\} 
\] and we define the corresponding vector subspace of $ \lalg $, 
\[
	V _{ \alpha, \beta } = \text{span} _{ \mathbb{ C} } \left\{  
	e ^{ \beta + e \alpha} ; \beta + e \alpha \in S_{ \alpha , \beta }\right\} 
\] 
Now consider the action of $ Sl ( 2) _{ \alpha } $ on 
$ V_{ \alpha, \beta } $ 
\begin{align*}
	[ h ^ \alpha, e ^{ \beta + e \alpha } ] &=  \frac{ 2 ( \alpha, \beta + e \alpha ) }{ 
	\left(  \alpha, \alpha  \right)  } e ^{ \beta + e \alpha }  \\
						&=  \left(  \frac{2 ( \alpha , \beta ) }{ ( \alpha, \alpha ) } 
						+ 2 e \right)  e ^{ \beta + e \alpha }  \\
\end{align*}
In addition, we have that 
\[
	[ e ^{ \pm \alpha } , e ^{ \beta + e \alpha} ] \begin{cases}
		e ^{ \beta + ( e \pm 1 ) \alpha } & \text{if } \beta + ( e \pm 1 ) \alpha \in \Phi \\
		0 & \text{otherwise}
	\end{cases}
\]  
Hence, we find that $ V_{ \alpha, \beta } $ is a representation 
space for some representation $ R $ of $ Sl ( 2 )_ \alpha $. 
Specifically, our weight set of $ R $  is given by 
\[
	S_{ R }  = \left\{  \frac{2 ( \alpha, \beta ) }{ ( \alpha, \alpha 0 ) } +  2 e ; 
	\beta + e \alpha \in \Phi \right\} 
\] R is finite dimensional, and irreducible.
This means that $ R  =R _{ \Lambda } $ for 
some highest weight $ \Lambda \in \mathbb{ Z} $, and 
$ S _{ \Lambda }  = \left\{   - \Lambda , \dots, \Lambda  \right\}  $. 
This means that we must have $ R \simeq R_{\Lambda } $,
for some $ \Lambda \in \mathbb{ Z} _{ \geq 0  } $. 
Hence, we must have $ e = n \in \mathbb{ Z} $, with 
$ n _ - \leq n \leq n _ + $, and $ n_{ \pm } \in \mathbb{ Z} $. 
We can match up our weights to give 
\begin{align*}
	- \Lambda &  = \frac{ 2 ( \alpha , \beta ) }{ ( \alpha, \alpha ) } + 2 n_ - \\
	\Lambda &=   2 \frac{ ( \alpha, \beta ) }{ ( \alpha, \alpha )   } + 2 n _ +   \\
\end{align*}
Adding this gives $ \frac{ 2 ( \alpha , \beta ) }{ ( \alpha , \alpha ) }  = - ( n _ + + n _ - ) $
\pagebreak 
